{"cells":[{"cell_type":"markdown","metadata":{"id":"ya3OG_8hui9k"},"source":["## 1. Introduction"]},{"cell_type":"markdown","metadata":{"id":"8YvHqaGJum5b"},"source":["In this notebook we will work on the presidents dataset using embeddings from a pre-trained mode.\n","\n","Our first approach will be to use the pre-trained model only for the embeddings, then use a classifier from sklearn for the actual task. We will be able to run this on CPU.\n","\n","Our second approach will be to fine-tune the pre-trained model by integrating it into a larger architecture and training that.\n","\n","**NOTE:** The first approach should be easily implemented. Need to think about how to treat class imbalance. We have 2 approaches as seen in the previous notebook: 1. ignore it; 2. under-sample by choosing representative entries (using k-means for example). We saw that with our previous encodings the under-sampling wasn't good. We would hope that the embeddings given by a pre-trained model will give good cluster centroids."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XH4dnQhwEczs"},"outputs":[],"source":["import codecs\n","import re\n","\n","def load_pres(fname):\n","    alltxts = []\n","    alllabs = []\n","    s=codecs.open(fname, 'r','utf-8') # pour rÃ©gler le codage\n","    while True:\n","        txt = s.readline()\n","        if(len(txt))<5:\n","            break\n","        #\n","        lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n","        txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n","        if lab.count('M') >0:\n","            alllabs.append(-1)\n","        else:\n","            alllabs.append(1)\n","        alltxts.append(txt)\n","    return alltxts,alllabs\n","\n","fname = \"./drive/MyDrive/Colab_Projects/RITAL/datasets/AFDpresidentutf8/corpus.tache1.learn.utf8.txt\"\n","pres_alltxts, pres_alllabs = load_pres(fname)"]},{"cell_type":"markdown","metadata":{"id":"6BDd4SibFM0J"},"source":["## 2. First approach: embedding + simple classifier"]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"executionInfo":{"elapsed":10596,"status":"ok","timestamp":1742752761876,"user":{"displayName":"Enache Tudor","userId":"06737917839099751724"},"user_tz":-60},"id":"OTkY1FSWFS63","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f32e4ea8-2c55-4867-d9d1-92f6c7ea0082"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import numpy as np\n","\n","model_name = \"camembert-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name).to(\"cuda\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":73,"status":"ok","timestamp":1742752761960,"user":{"displayName":"Enache Tudor","userId":"06737917839099751724"},"user_tz":-60},"id":"xgRhpuaMFdtm"},"outputs":[],"source":["# create a train-test split before embedding\n","# otherwise we will have data leakage\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","x_train, x_test, y_train, y_test = train_test_split(pres_alltxts, pres_alllabs, test_size=0.2, stratify=pres_alllabs, random_state=42)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68716,"status":"ok","timestamp":1742752830685,"user":{"displayName":"Enache Tudor","userId":"06737917839099751724"},"user_tz":-60},"id":"XAIueGOWFjiI","outputId":"6e7357d6-13f5-4715-cdca-7bbb57298c03"},"outputs":[{"output_type":"stream","name":"stderr","text":["Generating Train Embeddings with GPU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1436/1436 [00:52<00:00, 27.27it/s]\n","Generating Test Embeddings with GPU: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 359/359 [00:13<00:00, 27.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","          -1       0.74      0.39      0.51      1505\n","           1       0.91      0.98      0.95      9978\n","\n","    accuracy                           0.90     11483\n","   macro avg       0.83      0.68      0.73     11483\n","weighted avg       0.89      0.90      0.89     11483\n","\n"]}],"source":["def get_batch_embeddings(batch_texts):\n","    inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","    inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","\n","batch_size = 32\n","dataloader_train = DataLoader(x_train, batch_size=batch_size, shuffle=False)\n","dataloader_test = DataLoader(x_test, batch_size=batch_size, shuffle=False)\n","\n","embeddings_train = []\n","for batch in tqdm(dataloader_train, desc=\"Generating Train Embeddings with GPU\"):\n","    batch_embeddings = get_batch_embeddings(batch)\n","    embeddings_train.append(batch_embeddings)\n","\n","x_train_embedded = np.vstack(embeddings_train)\n","y_train = np.array(y_train)\n","\n","embeddings_test = []\n","for batch in tqdm(dataloader_test, desc=\"Generating Test Embeddings with GPU\"):\n","    batch_embeddings = get_batch_embeddings(batch)\n","    embeddings_test.append(batch_embeddings)\n","\n","x_test_embedded = np.vstack(embeddings_test)\n","y_test = np.array(y_test)\n","\n","logreg = LogisticRegression(max_iter=10000)\n","logreg.fit(x_train_embedded, y_train)\n","y_pred = logreg.predict(x_test_embedded)\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"O5RhtJEMRMgX"},"source":["## 3. Running on test data / creating submission"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1742752830705,"user":{"displayName":"Enache Tudor","userId":"06737917839099751724"},"user_tz":-60},"id":"fpAjro8RRP-X"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"V6A0Wvj-Y153"},"source":["## 4. Under-sampling"]},{"cell_type":"markdown","metadata":{"id":"-jS3slrIVnvr"},"source":["We propose two approaches to under-sampling the majority class:\n","1. random\n","2. clustering and identifying representatives"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":562,"status":"ok","timestamp":1742752831273,"user":{"displayName":"Enache Tudor","userId":"06737917839099751724"},"user_tz":-60},"id":"XCAeGJMRXqC3","outputId":"ed14d551-4031-4b44-9b8c-7b621a07993a"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","          -1       0.38      0.78      0.51      1505\n","           1       0.96      0.81      0.88      9978\n","\n","    accuracy                           0.80     11483\n","   macro avg       0.67      0.79      0.69     11483\n","weighted avg       0.88      0.80      0.83     11483\n","\n"]}],"source":["# 1st approach\n","# use logistic regression as a classifier\n","m = np.where(y_train == -1)[0]\n","c = np.where(y_train == 1)[0]\n","\n","c_undersampled = np.random.choice(c, size=len(m), replace=False)\n","\n","x_train_undersampled = np.vstack((x_train_embedded[m], x_train_embedded[c_undersampled]))\n","y_train_undersampled = np.hstack((y_train[m], y_train[c_undersampled]))\n","\n","logreg_under = LogisticRegression(max_iter=10000)\n","logreg_under.fit(x_train_undersampled, y_train_undersampled)\n","y_pred_under = logreg_under.predict(x_test_embedded)\n","print(classification_report(y_test, y_pred_under))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83099,"status":"ok","timestamp":1742752914377,"user":{"displayName":"Enache Tudor","userId":"06737917839099751724"},"user_tz":-60},"id":"F6cXgHFmaQFH","outputId":"71e7506d-5592-4159-a691-dd46619a3892"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","          -1       0.70      0.41      0.51      1505\n","           1       0.92      0.97      0.94      9978\n","\n","    accuracy                           0.90     11483\n","   macro avg       0.81      0.69      0.73     11483\n","weighted avg       0.89      0.90      0.89     11483\n","\n"]}],"source":["# 2nd approach\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","\n","n_cluster = 7523\n","pca = PCA(n_components=25)\n","embeddings_pca = pca.fit_transform(x_train_embedded[c])\n","kmeans = KMeans(n_clusters=n_cluster, random_state=42).fit(embeddings_pca)\n","cluster_centers = kmeans.cluster_centers_\n","\n","# find dataset points closest to cluster centers\n","distances = np.linalg.norm(embeddings_pca[:, None, :] - cluster_centers[None, :, :], axis=-1)\n","closest_indices = np.argmin(distances, axis=1)\n","\n","c_x_train_undersampled = x_train_embedded[c][closest_indices]\n","m_x_train_undersampled = x_train_embedded[m]\n","\n","x_train_undersampled = np.vstack((m_x_train_undersampled, c_x_train_undersampled))\n","y_train_undersampled = np.hstack((y_train[m], y_train[c][closest_indices]))\n","\n","logreg_cluster = LogisticRegression(max_iter=10000)\n","logreg_cluster.fit(x_train_undersampled, y_train_undersampled)\n","y_pred_cluster = logreg_cluster.predict(x_test_embedded)\n","print(classification_report(y_test, y_pred_cluster))"]},{"cell_type":"markdown","metadata":{"id":"0NspsoyTY5Np"},"source":["## 5. Fine-tuning BERT"]},{"cell_type":"markdown","metadata":{"id":"yLNtbTEbjp7W"},"source":["Fine-tuning a model is a fairly automated process (I hope) as there are library interfaces for all necessary functions.\n","\n","The one thing we will pay close attention to is our train-test split. In a separate notebook we remarked on the structure present in the training dataset. We will try to keep this structure in how we feed our data.\n","\n","More specifically we will:\n","1. split at the chunk level\n","2. feed chunks to the model\n","3. over-sample the minority class\n","4. alternate between speakers in training"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2316,"status":"ok","timestamp":1742752916679,"user":{"displayName":"Enache Tudor","userId":"06737917839099751724"},"user_tz":-60},"id":"5-_zojAVyEeL","outputId":"be60d0c9-17b8-451b-fb89-852f6728d67c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["! pip install datasets"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2871,"status":"ok","timestamp":1742752919552,"user":{"displayName":"Enache Tudor","userId":"06737917839099751724"},"user_tz":-60},"id":"NbnjQ0N2nK9v","outputId":"72305dc1-ee68-4087-8d1e-f6c32efab97a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of M chunks:  960\n","Number of C chunks:  789\n","Number of M sentences:  18117\n","Number of C sentences:  39760\n","(57877,)\n","(57877,)\n","(14582,)\n","(14582,)\n"]}],"source":["# load the chunks\n","import json\n","\n","with open(\"./drive/MyDrive/Colab_Projects/RITAL/chunks/presidents_M.json\", \"r\") as f:\n","    chunks_M = json.load(f)\n","\n","with open(\"./drive/MyDrive/Colab_Projects/RITAL/chunks/presidents_C.json\", \"r\") as f:\n","    chunks_C = json.load(f)\n","\n","# 1. do a train-test split on the chunks\n","# 2. over-sample M by tripling it\n","# 3. organize training dataset as: half M, half C, half M, half C\n","# 4. train with this version of the corpus\n","# IMPORTANT: check in the beginning how many chunks of each class we have, and how many sentences this corresponds to\n","\n","m_train, m_test = train_test_split(list(chunks_M.keys()), test_size=0.2, random_state=42)\n","c_train, c_test = train_test_split(list(chunks_C.keys()), test_size=0.2, random_state=42)\n","\n","m_train = m_train * 3\n","m_test = m_test * 3\n","\n","print(\"Number of M chunks: \", len(m_train))\n","print(\"Number of C chunks: \", len(c_train))\n","\n","sentences_m = 0\n","sentences_c = 0\n","for chunk in m_train:\n","  sentences_m += chunks_M[chunk]\n","for chunk in c_train:\n","  sentences_c += chunks_C[chunk]\n","\n","print(\"Number of M sentences: \", sentences_m)\n","print(\"Number of C sentences: \", sentences_c)\n","\n","m = len(m_train)\n","c = len(c_train)\n","chunks_train = m_train[:m // 2] + c_train[:c // 2] + m_train[m // 2:] + c_train[c // 2:]\n","chunks_test = m_test + c_test\n","\n","x_train = []\n","y_train = []\n","x_test = []\n","y_test = []\n","\n","for chunk in chunks_train:\n","  if chunk in chunks_M:\n","    length = chunks_M[chunk]\n","  else:\n","    length = chunks_C[chunk]\n","  chunk = int(chunk)\n","  for i in range(chunk, chunk + length):\n","    x_train.append(pres_alltxts[i])\n","    y_train.append((pres_alllabs[i] + 1) // 2)\n","\n","for chunk in chunks_test:\n","  if chunk in chunks_M:\n","    length = chunks_M[chunk]\n","  else:\n","    length = chunks_C[chunk]\n","  chunk = int(chunk)\n","  for i in range(chunk, chunk + length):\n","    x_test.append(pres_alltxts[i])\n","    y_test.append((pres_alllabs[i] + 1) // 2)\n","\n","x_train = np.array(x_train)\n","y_train = np.array(y_train)\n","x_test = np.array(x_test)\n","y_test = np.array(y_test)\n","print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)\n","\n","from datasets import Dataset\n","\n","train_dataset = Dataset.from_dict({\"text\": x_train, \"label\": y_train})\n","test_dataset = Dataset.from_dict({\"text\": x_test, \"label\": y_test})"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1082,"status":"ok","timestamp":1742752920651,"user":{"displayName":"Enache Tudor","userId":"06737917839099751724"},"user_tz":-60},"id":"tQbpPuLZyHoD","outputId":"7fc183d5-9d4d-4b31-f6cd-2cc825e0b17a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"execute_result","data":{"text/plain":["CamembertForSequenceClassification(\n","  (roberta): CamembertModel(\n","    (embeddings): CamembertEmbeddings(\n","      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): CamembertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x CamembertLayer(\n","          (attention): CamembertAttention(\n","            (self): CamembertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): CamembertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): CamembertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): CamembertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): CamembertClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":9}],"source":["# now let's fine-tune a model\n","from transformers import CamembertTokenizer, CamembertForSequenceClassification\n","import torch\n","\n","model_name = \"camembert-base\"\n","tokenizer = CamembertTokenizer.from_pretrained(model_name)\n","model = CamembertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","model.to(device)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2523,"status":"ok","timestamp":1742752923176,"user":{"displayName":"Enache Tudor","userId":"06737917839099751724"},"user_tz":-60},"id":"pSQtNdTG6BvS","outputId":"8b85c621-a679-4f90-920c-aaf063bc3654"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.29.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.14)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"]}],"source":["! pip install evaluate"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225,"referenced_widgets":["e128cb4a8341406799e0ae4522fe1fb3","fd12241718f646f89654172dab351706","d4c775783b1c46ea90d138469a052056","3bf50b5d9574453db936c8b2c7f5b9d0","5f9d924003fb430fac2c38ff15809e4d","cad6285dc79e4a3dad94c728de58e125","f31060b76de04818a800f85776a537cc","24e1685f90234bb98efa3c58aa45f7da","eed68c7ac5e8405989576cfc35a4cde6","ed2d88ebc36b4391838e5e8da2c6292f","836855a0933f473a8e306e1c68eb3ab8","d3d6c876e4784bd59e2a3826c01b9cdf","85dcf21c54ca42e1b5378bc5dadfdc04","5ef3ca82d2d8480087b9b5747a59b87d","64ed646e611d403cb7274c327a0e78ac","1863f5a507a34dac84c4f1823b2eb9b4","04008f5a964042beaf6e2b0c7adb283c","3f34a9ccdd574649bd32248561686d0b","ede4c69d4ea54f0f889a1fd53f2930f7","cb5bc1addb1c4d7695a3f3ecbe5fecca","9fa1d32e2abc425d96439c9eb2d8386a","c9da87d6154e4c6099d1d236d85bd36f"]},"collapsed":true,"id":"i-WHBWEJ5qIq","executionInfo":{"status":"ok","timestamp":1742752951881,"user_tz":-60,"elapsed":28702,"user":{"displayName":"Enache Tudor","userId":"06737917839099751724"}},"outputId":"a0174c8e-4145-4608-f673-6f286679de7f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/57877 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e128cb4a8341406799e0ae4522fe1fb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/14582 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3d6c876e4784bd59e2a3826c01b9cdf"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-11-a022a3150ce9>:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]}],"source":["def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","tokenized_train = train_dataset.map(tokenize_function, batched=True)\n","tokenized_test = test_dataset.map(tokenize_function, batched=True)\n","from transformers import TrainingArguments, Trainer\n","import evaluate\n","\n","accuracy = evaluate.load(\"accuracy\")\n","f1 = evaluate.load(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","  logits, labels = eval_pred\n","  predictions = np.argmax(logits, axis=-1)\n","  return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels),\n","          \"f1\": f1.compute(predictions=predictions, references=labels)}\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    logging_dir=\"./logs\",\n","    logging_steps=100,\n","    no_cuda=False,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_train,\n","    eval_dataset=tokenized_test,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=None,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":251},"id":"j44BRqLx6eNh","outputId":"25874c67-b368-475e-ade2-50fde513162b"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtudor-enache7\u001b[0m (\u001b[33mtudor-enache7-sorbonne-universit-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.8"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250323_180233-o02pbdhn</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/tudor-enache7-sorbonne-universit-/huggingface/runs/o02pbdhn' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/tudor-enache7-sorbonne-universit-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/tudor-enache7-sorbonne-universit-/huggingface' target=\"_blank\">https://wandb.ai/tudor-enache7-sorbonne-universit-/huggingface</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/tudor-enache7-sorbonne-universit-/huggingface/runs/o02pbdhn' target=\"_blank\">https://wandb.ai/tudor-enache7-sorbonne-universit-/huggingface/runs/o02pbdhn</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='7236' max='14470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 7236/14470 20:43 < 20:43, 5.82 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='1011' max='1823' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1011/1823 00:56 < 00:45, 17.75 it/s]\n","    </div>\n","    "]},"metadata":{}}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MiZbuI1vcHuN"},"outputs":[],"source":["# run model on testing dataset\n","from sklearn.metrics import classification_report\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import autocast\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","batch_size = 8\n","\n","x_test = list(x_test)\n","y_pred = []\n","for i in range(0, len(x_test), batch_size):\n","    batch_texts = x_test[i:i+batch_size]\n","    inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n","    inputs = {key: val.to(device) for key, val in inputs.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    logits = outputs.logits\n","    probs = F.softmax(logits, dim=-1).cpu().numpy()\n","    y_pred.extend(probs)\n","\n","y_pred = np.array(y_pred)\n","y_pred = np.argmax(y_pred, axis=1)\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"97hKTfroHkUE"},"outputs":[],"source":["# train model on whole dataset following the same training pattern\n","m = list(chunks_M.keys())\n","c = list(chunks_C.keys())\n","m = m * 3\n","\n","len_m = len(m)\n","len_c = len(c)\n","chunks = m[:len_m // 2] + c[:len_c // 2] + m[len_m // 2:] + c[len_c // 2:]\n","\n","x = []\n","y = []\n","for chunk in chunks:\n","  if chunk in chunks_M:\n","    length = chunks_M[chunk]\n","  else:\n","    length = chunks_C[chunk]\n","  chunk = int(chunk)\n","  for i in range(chunk, chunk + length):\n","    x.append(pres_alltxts[i])\n","    y.append((pres_alllabs[i] + 1) // 2)\n","x = np.array(x)\n","y = np.array(y)\n","\n","dataset = Dataset.from_dict({\"text\": x, \"label\": y})\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"no\",\n","    save_strategy=\"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=8,\n","    num_train_epochs=2,\n","    weight_decay=0.01\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator=None,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXhP_mMCNhuZ"},"outputs":[],"source":["# use this model to predict classes for the testing dataset\n","fname = \"./drive/MyDrive/Colab_Projects/RITAL/datasets/AFDpresidentutf8/corpus.tache1.test.utf8.txt\"\n","test, _ = load_pres(fname)\n","\n","tokenized_test = tokenizer(test, padding=True, truncation=True, return_tensors=\"pt\")\n","tokenized_test = {k: v.to(device) for k, v in tokenized_test.items()}\n","\n","model.eval()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","batch_size = 8\n","\n","y_pred = []\n","for i in range(0, len(test), batch_size):\n","    batch_texts = test[i:i+batch_size]\n","    inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n","    inputs = {key: val.to(device) for key, val in inputs.items()}\n","\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    logits = outputs.logits\n","    probs = F.softmax(logits, dim=-1).cpu().numpy()\n","    y_pred.extend(probs)\n","\n","y_pred = np.array(y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5jy2VxqiKXo"},"outputs":[],"source":["print(y_pred.shape)\n","print(y_pred[:, 0].shape)\n","np.save(\"./drive/MyDrive/Colab_Projects/RITAL/predictions/final.npy\", y_pred[:, 0])"]},{"cell_type":"markdown","metadata":{"id":"xyHphu8AY7oV"},"source":["## 6. Comparison of the three models"]},{"cell_type":"markdown","source":["Comparing the three models would have required creating the same train-test split for all of them. This would have required a bit more foresight on our part.\n","\n","Part of the reason we did not structure it this way is that the first BERT models where built when we still had not spent time understanding the paragraph structure of the dataset, while the BERT fine-tune was built with this in mind.\n","\n","Still, we keep the same split ration, so we can comment on our results."],"metadata":{"id":"djDSy9GZ-bMx"}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, roc_auc_score\n","from matplotlib.pyplot import plt\n","\n","fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n","auc = roc_auc_score(y_test, y_pred)\n","\n","plt.figure()\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title(f'ROC Curve of BERT')\n","plt.legend()\n","plt.savefig(\"drive/MyDrive/Colab_Projects/RITAL/plots/roc_curve_bert.png\")\n","plt.show()"],"metadata":{"id":"TyITU3ouJyD4"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"mount_file_id":"12tsVMyeXflIylXzIOcWs3NHF3ej2xuEa","authorship_tag":"ABX9TyMVEgy3xpNCvzBGPKzpm8hG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e128cb4a8341406799e0ae4522fe1fb3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd12241718f646f89654172dab351706","IPY_MODEL_d4c775783b1c46ea90d138469a052056","IPY_MODEL_3bf50b5d9574453db936c8b2c7f5b9d0"],"layout":"IPY_MODEL_5f9d924003fb430fac2c38ff15809e4d"}},"fd12241718f646f89654172dab351706":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cad6285dc79e4a3dad94c728de58e125","placeholder":"â€‹","style":"IPY_MODEL_f31060b76de04818a800f85776a537cc","value":"Map:â€‡100%"}},"d4c775783b1c46ea90d138469a052056":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24e1685f90234bb98efa3c58aa45f7da","max":57877,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eed68c7ac5e8405989576cfc35a4cde6","value":57877}},"3bf50b5d9574453db936c8b2c7f5b9d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed2d88ebc36b4391838e5e8da2c6292f","placeholder":"â€‹","style":"IPY_MODEL_836855a0933f473a8e306e1c68eb3ab8","value":"â€‡57877/57877â€‡[00:20&lt;00:00,â€‡2982.86â€‡examples/s]"}},"5f9d924003fb430fac2c38ff15809e4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cad6285dc79e4a3dad94c728de58e125":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f31060b76de04818a800f85776a537cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24e1685f90234bb98efa3c58aa45f7da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eed68c7ac5e8405989576cfc35a4cde6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed2d88ebc36b4391838e5e8da2c6292f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"836855a0933f473a8e306e1c68eb3ab8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3d6c876e4784bd59e2a3826c01b9cdf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85dcf21c54ca42e1b5378bc5dadfdc04","IPY_MODEL_5ef3ca82d2d8480087b9b5747a59b87d","IPY_MODEL_64ed646e611d403cb7274c327a0e78ac"],"layout":"IPY_MODEL_1863f5a507a34dac84c4f1823b2eb9b4"}},"85dcf21c54ca42e1b5378bc5dadfdc04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04008f5a964042beaf6e2b0c7adb283c","placeholder":"â€‹","style":"IPY_MODEL_3f34a9ccdd574649bd32248561686d0b","value":"Map:â€‡100%"}},"5ef3ca82d2d8480087b9b5747a59b87d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ede4c69d4ea54f0f889a1fd53f2930f7","max":14582,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cb5bc1addb1c4d7695a3f3ecbe5fecca","value":14582}},"64ed646e611d403cb7274c327a0e78ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fa1d32e2abc425d96439c9eb2d8386a","placeholder":"â€‹","style":"IPY_MODEL_c9da87d6154e4c6099d1d236d85bd36f","value":"â€‡14582/14582â€‡[00:05&lt;00:00,â€‡2943.52â€‡examples/s]"}},"1863f5a507a34dac84c4f1823b2eb9b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04008f5a964042beaf6e2b0c7adb283c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f34a9ccdd574649bd32248561686d0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ede4c69d4ea54f0f889a1fd53f2930f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb5bc1addb1c4d7695a3f3ecbe5fecca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9fa1d32e2abc425d96439c9eb2d8386a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9da87d6154e4c6099d1d236d85bd36f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}